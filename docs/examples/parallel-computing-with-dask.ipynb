{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Guide for Parallelizing xCDAT Operations with Dask\n",
    "\n",
    "Author: [Tom Vo](https://github.com/tomvothecoder)\n",
    "\n",
    "Date: 02/27/24\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook serves as a general guide for parallel computing with xCDAT. It covers the\n",
    "following topics:\n",
    "\n",
    "- Basics of Dask Arrays\n",
    "- General Dask Best Practice\n",
    "- How to use Dask with Xarray\n",
    "- How to use Dask with xCDAT, including real-world examples and performance metrics\n",
    "- Dask Schedulers and using a local distributed scheduler for more resource-intensive needs\n",
    "\n",
    "_The data used in the code examples can be found through the [Earth System Grid Federation (ESGF) search portal](https://aims2.llnl.gov/search)._\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "Create an Anaconda environment for this notebook using the command below:\n",
    "\n",
    "```bash\n",
    "conda create -n xcdat_dask_guide -c conda-forge python xarray dask xcdat flox matplotlib nc-time-axis jupyter jupyter-server-proxy\n",
    "```\n",
    "\n",
    "- [`flox`](https://flox.readthedocs.io/en/latest/) is a package that is used to improve the xarray `groupby()` performance by\n",
    "  making it parallelizable.\n",
    "- `matplotlib` is an optional dependency required for plotting with xarray\n",
    "- [`nc-time-axis`](https://nc-time-axis.readthedocs.io/en/latest/) is an optional dependency required for `matplotlib` to plot `cftime` coordinates\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Dask Arrays\n",
    "\n",
    "- **Dask divides arrays** into many small pieces, called **\"chunks\"** (each presumed to be small enough to fit into memory)\n",
    "- Dask Array **operations are lazy**\n",
    "  - Operations **queue** up a series of tasks mapped over blocks\n",
    "  - No computation is performed until values need to be computed (hence \"lazy\")\n",
    "  - Data is loaded into memory and **computation** is performed in **streaming fashion**, **block-by-block**\n",
    "- Computation is controlled by multi-processing or thread pool\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../_static/dask-array.png\" alt=\"Dask Array\" style=\"display: inline-block; width:300px;\">\n",
    "</div>\n",
    "\n",
    "Source: <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html</cite>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Dask Best Practices\n",
    "\n",
    "- [Use NumPy](https://docs.dask.org/en/stable/array-best-practices.html#use-numpy)\n",
    "  - If your **data fits comfortably in RAM and you are not performance bound**, then using **NumPy might be the right choice**. Dask adds another layer of complexity which may get in the way.\n",
    "  - If you are just looking for **speedups rather than scalability** then you may want to **consider a project like [Numba](https://numba.pydata.org/)**\n",
    "- [Select a good chunk size](https://docs.dask.org/en/stable/array-best-practices.html#select-a-good-chunk-size)\n",
    "  - A **common performance problem among Dask Array users** is that they have chosen a **chunk size** that is either **too small** (leading to lots of overhead) or **poorly aligned with their data** (leading to inefficient reading).\n",
    "- [Orient your chunks](https://docs.dask.org/en/stable/array-best-practices.html#orient-your-chunks)\n",
    "  - When reading data you should **align your chunks with your storage format**.\n",
    "- [Avoid Oversubscribing Threads](https://docs.dask.org/en/stable/array-best-practices.html#avoid-oversubscribing-threads)\n",
    "  - **By default Dask will run as many concurrent tasks as you have logical cores.** It assumes that **each task** will consume about **one core**. However, many array-computing libraries are themselves multi-threaded, which can cause contention and low performance.\n",
    "- [Consider Xarray](hhttps://docs.dask.org/en/stable/array-best-practices.html#consider-xarray)\n",
    "  - The **Xarray package wraps** around **Dask Array**, and so offers the **same scalability**, but also adds **convenience when dealing with complex datasets**.\n",
    "- [Build your own Operations](https://docs.dask.org/en/stable/array-best-practices.html#build-your-own-operations)\n",
    "  - Often we want to perform computations for which there is no exact function in Dask Array. In these cases we may be able to use some of the more generic functions to build our own.\n",
    "\n",
    "Source: <cite>https://docs.dask.org/en/stable/array-best-practices.html#best-practices</cite>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use Dask with Xarray\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why does Xarray integrate with Dask?\n",
    "  > Xarray integrates with Dask to support parallel computations and streaming computation\n",
    "  > on datasets that donâ€™t fit into memory. Currently, Dask is an entirely optional feature\n",
    "  > for xarray. However, the benefits of using Dask are sufficiently strong that Dask may\n",
    "  > become a required dependency in a future version of xarray.\n",
    "  >\n",
    "  > &mdash; <cite>https://docs.xarray.dev/en/stable/use\n",
    "- Which Xarray features support Dask?\n",
    "\n",
    "  > Nearly all existing xarray methods (including those for indexing, computation,\n",
    "  > concatenating and grouped operations) have been extended to work automatically with\n",
    "  > Dask arrays. When you load data as a Dask array in an xarray data structure, almost\n",
    "  > all xarray operations will keep it as a Dask array; when this is not possible, they\n",
    "  > will raise an exception rather than unexpectedly loading data into memory.\n",
    "  >\n",
    "  > &mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#using-dask-with-xarray</cite>\n",
    "\n",
    "- What is the default Dask behavior for distributing work on compute hardware?\n",
    "\n",
    "  > By default, dask uses its multi-threaded scheduler, which distributes work across\n",
    "  > multiple cores and allows for processing some datasets that do not fit into memory.\n",
    "  > For running across a cluster, [setup the distributed scheduler](https://docs.dask.org/en/latest/setup.html).\n",
    "  >\n",
    "  > &mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#using-dask-with-xarray</cite>\n",
    "\n",
    "- How do I use Dask arrays in an `xarray.Dataset`?\n",
    "\n",
    "  > The usual way to create a Dataset filled with Dask arrays is to load the data from a\n",
    "  > netCDF file or files. You can do this by supplying a `chunks` argument to [open_dataset()](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html#xarray.open_dataset)\n",
    "  > or using the [open_mfdataset()](https://docs.xarray.dev/en/stable/generated/xarray.open_mfdataset.html#xarray.open_mfdataset) function.\n",
    "\n",
    "- What happens if I don't specify `chunks` with `open_mfdataset()`?\n",
    "\n",
    "  > `open_mfdataset()` called without `chunks` argument will return dask arrays with\n",
    "  > chunk sizes equal to the individual files. Re-chunking the dataset after creation\n",
    "  > with `ds.chunk()` will lead to an ineffective use of memory and is not recommended.\n",
    "  >\n",
    "  > &mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#reading-and-writing-data</cite>\n",
    "\n",
    "- Are there any optimizations tips for working with Dask and Xarray?\n",
    "\n",
    "  - We HIGHLY recommend checking out the [Optimization Tips](https://docs.xarray.dev/en/stable/user-guide/dask.html#optimization-tips) section if you are using Dask with Xarray.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Best Practices\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For performance, a good choice of `chunks` follows the following rules:\n",
    ">\n",
    "> 1. A chunk should be small enough to fit comfortably in memory. We'll\n",
    ">    have many chunks in memory at once\n",
    "> 2. A chunk must be large enough so that computations on that chunk take\n",
    ">    significantly longer than the 1ms overhead per task that Dask scheduling\n",
    ">    incurs. A task should take longer than 100ms\n",
    "> 3. Chunk sizes between 10MB-1GB are common, depending on the availability of\n",
    ">    RAM and the duration of computations\n",
    "> 4. Chunks should align with the computation that you want to do.\n",
    ">    - For example, if you plan to frequently slice along a particular dimension,\n",
    ">      then it's more efficient if your chunks are aligned so that you have to\n",
    ">      touch fewer chunks. If you want to add two arrays, then its convenient if\n",
    ">      those arrays have matching chunks patterns\n",
    "> 5. Chunks should align with your storage, if applicable.\n",
    ">    - Array data formats are often chunked as well. When loading or saving data,\n",
    ">      if is useful to have Dask array chunks that are aligned with the chunking\n",
    ">      of your storage, often an even multiple times larger in each direction\n",
    ">\n",
    "> &mdash; <cite>https://docs.dask.org/en/latest/array-chunks.html</cite>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and Performance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The chunks parameter has critical performance implications when using Dask arrays. If\n",
    "> your chunks are too small, queueing up operations will be extremely slow, because Dask\n",
    "> will translate each operation into a huge number of operations mapped across chunks.\n",
    "> Computation on Dask arrays with small chunks can also be slow, because each operation\n",
    "> on a chunk has some fixed overhead from the Python interpreter and the Dask task\n",
    "> executor.\n",
    ">\n",
    "> Conversely, if your chunks are too big, some of your computation may be wasted, because\n",
    "> Dask only computes results one chunk at a time.\n",
    ">\n",
    "> A good rule of thumb is to create arrays with a minimum chunksize of at least one\n",
    "> million elements (e.g., a 1000x1000 > matrix). With large arrays (10+ GB), the cost of\n",
    "> queueing up Dask operations can be noticeable, and you may need even > larger\n",
    "> chunksizes.\n",
    ">\n",
    "> &mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#chunking-and-performance</cite>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Tip: You can let Dask figure out the optimal chunk size\n",
    "\n",
    "Dask Arrays can look for a `.chunks` attribute and use that to provide a good chunking.\n",
    "This helps prevent users from specifying \"too many chunks\" and \"too few chunks\" which\n",
    "can lead to performance issues.\n",
    "\n",
    "To do this in `xarray.open_dataset()` and `xarray.open_mfdataset()`, specify:\n",
    "\n",
    "1. `chunks={\"time\": \"auto\"}`: auto-scale the specified dimension to get to accommodate ideal chunk sizes\n",
    "   - replace `\"time\"` and/or add additional dims to the dictionary for auto-scaling\n",
    "2. `chunks=\"auto\"`: allow chunking _all_ dimensions to accommodate ideal chunk sizes\n",
    "\n",
    "_Disclaimer: Dask's automatic chunking scheme might not be optimal for some datasets\n",
    "and/or computational operations._\n",
    "\n",
    "> &mdash; <cite>https://docs.dask.org/en/latest/array-chunks.html#automatic-chunking</cite>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and Parallelism with Xarray\n",
    "\n",
    "The code example below demonstrates chunking a dataset in Xarray. By default, dask uses its multi-threaded scheduler, which distributes work across multiple cores and allows for processing some datasets that do not fit into memory.\n",
    "\n",
    "If you are interested in using a distributed scheduler (local or cluster) for more resource-intensive computational operations, refer to the last cell of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import logging\n",
    "\n",
    "# Silence flox logger info messages.\n",
    "logger = logging.getLogger(\"flox\")\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Disclaimer: The dataset used in the example is only a few hundred MBs to make\n",
    "# downloading the file quick. A file this small should normally **NOT** be\n",
    "# chunked since computational performance will most likely suffer.\n",
    "filepath = \"http://esgf.nci.org.au/thredds/dodsC/master/CMIP6/CMIP/CSIRO/ACCESS-ESM1-5/historical/r10i1p1f1/Amon/tas/gn/v20200605/tas_Amon_ACCESS-ESM1-5_historical_r10i1p1f1_gn_185001-201412.nc\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk the time dimension by 10. Alternatively, let Dask auto-scale all dimensions to get a good chunk size using `chunks=\"auto\"`, which references the `.chunks` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(filepath, chunks={\"time\": \"10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a daily average using the `groupby` API.\n",
    "\n",
    "`flox` must be installed to make this API parallelizable and Xarray will use `flox` by\n",
    "default if it is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_daily_avg = (\n",
    "    ds[\"tas\"].groupby(ds.time.dt.day).mean(method=\"cohorts\", engine=\"flox\")\n",
    ")\n",
    "\n",
    "# `.compute()` will trigger the computation. This also happens automatically\n",
    "# when writing out the data to a file.\n",
    "# tas_daily_avg.compute()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and Parallelism with xCDAT\n",
    "\n",
    "Many core [xCDAT computational APIs](https://xcdat.readthedocs.io/en/latest/api.html#methods),\n",
    "including spatial averaging and temporal averaging, inherit Xarray's Dask support by operating on `xarray.Dataset` objects and making calls to parallelized Xarray APIs.\n",
    "\n",
    "**Just chunk the xarray.Dataset object** before **calling any of the parallelizable xCDAT APIs**.\n",
    "\n",
    "Here's an example with xCDAT's `temporal.average()` API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xcdat as xc\n",
    "\n",
    "filepath = \"http://esgf.nci.org.au/thredds/dodsC/master/CMIP6/CMIP/CSIRO/ACCESS-ESM1-5/historical/r10i1p1f1/Amon/tas/gn/v20200605/tas_Amon_ACCESS-ESM1-5_historical_r10i1p1f1_gn_185001-201412.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = xc.open_dataset(filepath, chunks={\"time\": \"10\"})\n",
    "tas_daily_avg2 = ds2.temporal.average(\"tas\")\n",
    "\n",
    "# `.compute()` will trigger the computation. This also happens automatically\n",
    "# when writing out the data to a file.\n",
    "# tas_daily_avg2.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Resources\n",
    "\n",
    "Now that you have a high-level introduction to Dask with Xarray, here are more resources\n",
    "to level up your knowledge.\n",
    "\n",
    "- [Official Xarray Parallel Computing with Dask Guide](https://docs.xarray.dev/en/stable/user-guide/dask.html)\n",
    "- [Official Xarray Parallel Computing with Dask Jupyter Notebook Tutorial](https://tutorial.xarray.dev/intermediate/xarray_and_dask.html)\n",
    "- [Official Dask guide for Xarray with Dask Arrays](https://examples.dask.org/xarray.html)\n",
    "- [Dask blog post: \"Choosing good chunk sizes in Dask\"](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Task Scheduling\n",
    "\n",
    "> All of the large-scale Dask collections like Dask Array, Dask DataFrame, and Dask Bag and the fine-grained APIs like delayed and futures generate task graphs where each node in the graph is a normal Python function and edges between nodes are normal Python objects that are created by one task as outputs and used as inputs in another task. After Dask generates these task graphs, it needs to execute them on parallel hardware. This is the job of a task scheduler. Different task schedulers exist, and each will consume a task graph and compute the same result, but with different performance characteristics.\n",
    "> Dask has two families of task schedulers:\n",
    ">\n",
    "> 1.  **Single-machine scheduler**: This scheduler provides basic features on a local process or thread pool. This scheduler was made first and is the default. It is simple and cheap to use, although it can only be used on a single machine and does not scale\n",
    ">\n",
    "> 2.  **Distributed scheduler**: This scheduler is more sophisticated, offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../_static/dask-overview-schedulers.svg\" alt=\"Dask Schedulers\" style=\"display: inline-block;\">\n",
    "</div>\n",
    "\n",
    "Source: <cite>https://docs.dask.org/en/stable/scheduling.html</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Dask Distributed Scheduler (local)\n",
    "\n",
    "Documentation: https://docs.dask.org/en/stable/scheduling.html#dask-distributed-local\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xarray is setup to use Dask's default single-machine, multi-threaded scheduler.\n",
    "- However, Dask advises uses to use the Dask distributed scheduler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup the Dask Client for the local distributed scheduler\n",
    "\n",
    "Xarray will automatically use the Dask Client when calling `.compute()` or `.load()`\n",
    "to trigger queued up operations in the Dask task graph.\n",
    "\n",
    "You can configure the Dask Client (e.g., memory limit) to your needs using the links:\n",
    "\n",
    "- https://distributed.dask.org/en/latest/api.html#client\n",
    "- https://distributed.dask.org/en/latest/api.html#distributed.LocalCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Open the Dask Dashboard UI\n",
    "\n",
    "The Dask distributed scheduler provides an interactive dashboard containing many plots\n",
    "and tables with live information.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../_static/dask-dashboard-example.png\" alt=\"Dask Dashboard UI Example\" style=\"display: inline-block; width:800px;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the link to the dashboard\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run an Xarray/xCDAT computation while viewing the dashboards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 15:09:31,004 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vo13/miniforge3/envs/xcdat_dask_guide/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vo13/miniforge3/envs/xcdat_dask_guide/lib/python3.12/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vo13/miniforge3/envs/xcdat_dask_guide/lib/python3.12/site-packages/distributed/utils_comm.py\", line 455, in retry_operation\n",
      "    return await retry(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/vo13/miniforge3/envs/xcdat_dask_guide/lib/python3.12/site-packages/distributed/utils_comm.py\", line 434, in retry\n",
      "    return await coro()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/vo13/miniforge3/envs/xcdat_dask_guide/lib/python3.12/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vo13/miniforge3/envs/xcdat_dask_guide/lib/python3.12/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vo13/miniforge3/envs/xcdat_dask_guide/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/vo13/miniforge3/envs/xcdat_dask_guide/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:49538 remote=tcp://127.0.0.1:38134>: Stream is closed\n"
     ]
    }
   ],
   "source": [
    "tas_daily_avg3 = ds.temporal.average(\"tas\")\n",
    "\n",
    "# .compute() will trigger the computation. This also happens automatically\n",
    "# when writing out the data to a file.\n",
    "tas_daily_avg3.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there cases where xCDAT loads Dask arrays into memory?\n",
    "\n",
    "As of `xarray=2023.5.0`, Xarray does not support updating/setting multi-dimensional dask\n",
    "arrays. The followin error is raised if this is attempted: `xarray can't set arrays with multiple array indices to dask yet`.\n",
    "\n",
    "As a workaround, xCDAT loads coordinate bounds into memory if they are multi-dimensional\n",
    "Dask arrays before performing operations or computations. This loading occurs in the\n",
    "following APIs:\n",
    "\n",
    "- `xcdat.axis.swap_lon_axis`\n",
    "  - swapping longitude axis orientation\n",
    "  - aligning longitude bounds to (0, 360) axis\n",
    "- `xarray.Dataset.spatial.average`\n",
    "  - generating weights using lat/lon coordinate bounds\n",
    "  - swapping longitude axis orientation\n",
    "  - scaling domain bounds to a specified region\n",
    "- `xcdat.Dataset.temporal.<average|group_average|climatology|departures>`\n",
    "  - generating weights using time coordinate bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcdat_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
