{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Guide for Parallelizing xCDAT Operations with Dask\n",
    "\n",
    "Author: [Tom Vo](https://github.com/tomvothecoder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook serves as a general guide for parallel computing with xCDAT. It covers the\n",
    "following topics:\n",
    "\n",
    "- Basics of Dask Arrays\n",
    "- General Dask Best Practice\n",
    "- How to use Dask with Xarray\n",
    "- How to use Dask with xCDAT, including real-world examples and performance metrics\n",
    "- Dask Schedulers and using a local distributed scheduler for more resource-intensive needs\n",
    "\n",
    "_The data used in the code examples can be found through the [Earth System Grid Federation (ESGF) search portal](https://aims2.llnl.gov/search)._\n",
    "\n",
    "### More Resources\n",
    "\n",
    "To learn more in-depth about Dask and Xarray, please check these resources out:\n",
    "\n",
    "- [Official Xarray Parallel Computing with Dask Guide](https://docs.xarray.dev/en/stable/user-guide/dask.html)\n",
    "- [Official Xarray Parallel Computing with Dask Jupyter Notebook Tutorial](https://tutorial.xarray.dev/intermediate/xarray_and_dask.html)\n",
    "- [Official Dask guide for Xarray with Dask Arrays](https://examples.dask.org/xarray.html)\n",
    "- [Project Pythia: Dask Arrays with Xarray](https://foundations.projectpythia.org/core/xarray/dask-arrays-xarray.html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "Create an Anaconda environment for this notebook using the command below. You can\n",
    "substitute `conda` with `mamba` if you are using Mamba instead.\n",
    "\n",
    "```bash\n",
    "conda create -n xcdat_dask_guide -c conda-forge python xarray netcdf4 dask xcdat flox matplotlib nc-time-axis jupyter jupyter-server-proxy\n",
    "```\n",
    "\n",
    "- [flox](https://flox.readthedocs.io/en/latest/) is a package that is used to improve the xarray `groupby()` performance by making it parallelizable.\n",
    "- [matplotlib](https://matplotlib.org/) is an optional dependency required for plotting with xarray.\n",
    "- [nc-time-axis](https://nc-time-axis.readthedocs.io/en/latest/) is an optional dependency required for `matplotlib` to plot `cftime` coordinates.\n",
    "- [jupyter-server-proxy](https://github.com/jupyterhub/jupyter-server-proxy) is a package used\n",
    "  for co-locating the Jupyter server with the Dask Scheduler so that they share the same port,\n",
    "  allowing for the Dask dashboard to route the connection to Jupyter (and vice versa).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Best Practices\n",
    "\n",
    "- **Use NumPy**\n",
    "  - If your data fits comfortably in RAM and you are not performance bound, then using NumPy might be the right choice.\n",
    "  - Dask adds another layer of complexity which may get in the way.\n",
    "  - If you are just looking for speedups rather than scalability then you may want to consider a project like [Numba](https://numba.pydata.org/)\n",
    "- **Select a good chunk size**\n",
    "  - A common performance problem among Dask Array users is that they have chosen a chunk size that is either too small (leading to lots of overhead) or poorly aligned with their data (leading to inefficient reading).\n",
    "- Orient your chunks\n",
    "  - When reading data you should align your chunks with your storage format.\n",
    "- **Avoid Oversubscribing Threads**\n",
    "  - By default Dask will run as many concurrent tasks as you have logical cores. It assumes that each task will consume about one core. However, many array-computing libraries are themselves multi-threaded, which can cause contention and low performance.\n",
    "- **Consider Xarray**\n",
    "  - The Xarray package wraps around Dask Array, and so offers the same scalability, but also adds convenience when dealing with complex datasets\n",
    "- **Build your own Operations**\n",
    "  - Often we want to perform computations for which there is no exact function in Dask Array. In these cases we may be able to use some of the more generic functions to build our own.\n",
    "\n",
    "&mdash; <cite>https://docs.dask.org/en/stable/array-best-practices.html#best-practices</cite>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics of Dask Arrays\n",
    "\n",
    "- **Dask divides arrays** into many small pieces, called **\"chunks\"** (each presumed to be small enough to fit into memory)\n",
    "- Dask Array **operations are lazy**\n",
    "  - Operations **queue** up a series of tasks mapped over blocks\n",
    "  - No computation is performed until values need to be computed (hence \"lazy\")\n",
    "  - Data is loaded into memory and **computation** is performed in **streaming fashion**, **block-by-block**\n",
    "- Computation is controlled by multi-processing or thread pool\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../_static/dask-array.png\" alt=\"Dask Array\" style=\"display: inline-block; width:300px;\">\n",
    "</div>\n",
    "\n",
    "&mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html</cite>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xarray and Dask\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../_static/xarray-logo.png\" alt=\"xarray logo\" style=\"display: inline-block; margin-right: 50px; width:400px;\">\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does Xarray integrate with Dask?**\n",
    "\n",
    "> Xarray integrates with Dask to support parallel computations and streaming computation\n",
    "> on datasets that donâ€™t fit into memory. Currently, Dask is an entirely optional feature\n",
    "> for xarray. However, the benefits of using Dask are sufficiently strong that Dask may\n",
    "> become a required dependency in a future version of xarray.\n",
    ">\n",
    "> &mdash; <cite>https://docs.xarray.dev/en/stable/use\n",
    "\n",
    "**Which Xarray features support Dask?**\n",
    "\n",
    "> Nearly all existing xarray methods (including those for indexing, computation,\n",
    "> concatenating and grouped operations) have been extended to work automatically with\n",
    "> Dask arrays. When you load data as a Dask array in an xarray data structure, almost\n",
    "> all xarray operations will keep it as a Dask array; when this is not possible, they\n",
    "> will raise an exception rather than unexpectedly loading data into memory.\n",
    ">\n",
    "> &mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#using-dask-with-xarray</cite>\n",
    "\n",
    "**What is the default Dask behavior for distributing work on compute hardware**\n",
    "\n",
    "> By default, dask uses its multi-threaded scheduler, which distributes work across\n",
    "> multiple cores and allows for processing some datasets that do not fit into memory.\n",
    "> For running across a cluster, [setup the distributed scheduler](https://docs.dask.org/en/latest/setup.html).\n",
    ">\n",
    "> &mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#using-dask-with-xarray</cite>\n",
    "\n",
    "**How do I use Dask arrays in an `xarray.Dataset`**\n",
    "\n",
    "> The usual way to create a Dataset filled with Dask arrays is to load the data from a\n",
    "> netCDF file or files. You can do this by supplying a `chunks` argument to [open_dataset()](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html#xarray.open_dataset)\n",
    "> or using the [open_mfdataset()](https://docs.xarray.dev/en/stable/generated/xarray.open_mfdataset.html#xarray.open_mfdataset) function.\n",
    "\n",
    "**What happens if I don't specify `chunks` with `open_mfdataset()`**\n",
    "\n",
    "> `open_mfdataset()` called without `chunks` argument will return dask arrays with\n",
    "> chunk sizes equal to the individual files. Re-chunking the dataset after creation\n",
    "> with `ds.chunk()` will lead to an ineffective use of memory and is not recommended.\n",
    ">\n",
    "> &mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#reading-and-writing-data</cite>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's learn about chunking arrays\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For performance, a good choice of `chunks` follows the following rules:\n",
    ">\n",
    "> 1. A chunk should be small enough to fit comfortably in memory. We'll\n",
    ">    have many chunks in memory at once\n",
    "> 2. A chunk must be large enough so that computations on that chunk take\n",
    ">    significantly longer than the 1ms overhead per task that Dask scheduling\n",
    ">    incurs. A task should take longer than 100ms\n",
    "> 3. Chunk sizes between 10MB-1GB are common, depending on the availability of\n",
    ">    RAM and the duration of computations\n",
    "> 4. Chunks should align with the computation that you want to do.\n",
    ">    - For example, if you plan to frequently slice along a particular dimension,\n",
    ">      then it's more efficient if your chunks are aligned so that you have to\n",
    ">      touch fewer chunks. If you want to add two arrays, then its convenient if\n",
    ">      those arrays have matching chunks patterns\n",
    "> 5. Chunks should align with your storage, if applicable.\n",
    ">    - Array data formats are often chunked as well. When loading or saving data,\n",
    ">      if is useful to have Dask array chunks that are aligned with the chunking\n",
    ">      of your storage, often an even multiple times larger in each direction\n",
    ">\n",
    "> &mdash; <cite>https://docs.dask.org/en/latest/array-chunks.html</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking with Xarray\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `chunks` parameter has critical performance implications when using Dask arrays.\n",
    "\n",
    "- **If your chunks are too small**, queueing up operations will be extremely slow.\n",
    "\n",
    "  - Dask will translate each operation into a huge number of operations mapped across chunks.\n",
    "  - Computation on Dask arrays with small chunks can also be slow, because each operation on a chunk has some fixed overhead from the Python interpreter and the Dask task executor.\n",
    "\n",
    "- **If your chunks are too big**, some of your computation may be wasted. Dask only computes results one chunk at a time.\n",
    "\n",
    "&mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#chunking-and-performance</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good rule of thumb\n",
    "\n",
    "**Create arrays with a minimum chunksize of at least one million elements (e.g., a 1000x1000 > matrix).**\n",
    "\n",
    "**With large arrays (10+ GB)**, the cost of queueing up Dask operations can be noticeable and **you may need even > larger chunksizes**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or let Dask try to figure out chunking for you\n",
    "\n",
    "Dask Arrays can look for a `.chunks` attribute and use that to provide a good chunking.\n",
    "This can help prevent users from specifying \"too many chunks\" and \"too few chunks\" which\n",
    "can lead to performance issues.\n",
    "\n",
    "To do this in `open_dataset()`/`open_mfdataset()`, specify `chunks` on a specific dimension(s) or all dimensions, as shown below:\n",
    "\n",
    "1. `chunks={\"time\": \"auto\"}` - auto-scale the specified dimension(s) to get to accommodate ideal chunk sizes. In this example, replace `\"time\"` and/or add additional dims to the dictionary for additional auto-scaling.\n",
    "2. `chunks=\"auto\"` - allow chunking _all_ dimensions to accommodate ideal chunk sizes\n",
    "\n",
    "&mdash; <cite>https://docs.dask.org/en/latest/array-chunks.html#automatic-chunking</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DISCLAIMER: Although Dask's chunk auto-scaling tries its best to optimally align chunks to the ideal sizes, the auto-scaling might not always be optimal. For these cases, it is recommended\n",
    "> that you manually chunk for ideal sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example - Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import logging\n",
    "\n",
    "# Silence flox logger info messages.\n",
    "logger = logging.getLogger(\"flox\")\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Disclaimer: The dataset used in the example is only a few hundred MBs to make\n",
    "# downloading the file quick. A file this small should normally **NOT** be\n",
    "# chunked since computational performance will most likely suffer.\n",
    "filepath = \"http://esgf.nci.org.au/thredds/dodsC/master/CMIP6/CMIP/CSIRO/ACCESS-ESM1-5/historical/r10i1p1f1/Amon/tas/gn/v20200605/tas_Amon_ACCESS-ESM1-5_historical_r10i1p1f1_gn_185001-201412.nc\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example - Parallelism with xCDAT + Dask\n",
    "\n",
    "The code example below demonstrates chunking a dataset in Xarray and grouping the data\n",
    "in parallel across chunks.\n",
    "\n",
    "**By default, dask uses its multi-threaded scheduler**, which distributes work across multiple cores and allows for processing some datasets that do not fit into memory.\n",
    "\n",
    "If you are interested in using a distributed scheduler (local or cluster) for more resource-intensive computational operations, there is more information below in this notebook.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're letting Dask auto-scale all dimensions to get a good chunk size using `chunks=\"auto\"`, which references the `.chunks` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(filepath, chunks=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a daily average using the `groupby` API.\n",
    "\n",
    "`flox` must be installed to make this API parallelizable and Xarray will use `flox` by\n",
    "default if it is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_daily_avg_xr = (\n",
    "    ds[\"tas\"].groupby(ds.time.dt.year).mean(method=\"cohorts\", engine=\"flox\")\n",
    ")\n",
    "\n",
    "tas_daily_avg_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.load()` or `.compute()` will trigger the computation, which loads the data into\n",
    "memory. This also automatically happens when writing out the data to a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_daily_avg_xr.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example - Parallelism with xCDAT + Dask\n",
    "\n",
    "Many core [xCDAT computational APIs](https://xcdat.readthedocs.io/en/latest/api.html#methods),\n",
    "including spatial averaging and temporal averaging, inherit Xarray's Dask support by operating on `xarray.Dataset` objects and making calls to parallelized Xarray APIs.\n",
    "\n",
    "**Just chunk the xarray.Dataset object as you normally would before calling any of the parallelizable xCDAT APIs**.\n",
    "\n",
    "Here's an example with xCDAT's `temporal.group_average()` API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_daily_avg_xc = ds.temporal.group_average(\"tas\", freq=\"month\")\n",
    "\n",
    "tas_daily_avg_xc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.load()` or `.compute()` will trigger the computation, which loads the data into\n",
    "memory. This also automatically happens when writing out the data to a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_daily_xc.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Task Scheduling\n",
    "\n",
    "> All of the large-scale Dask collections like Dask Array, Dask DataFrame, and Dask Bag and the fine-grained APIs like delayed and futures generate task graphs where each node in the graph is a normal Python function and edges between nodes are normal Python objects that are created by one task as outputs and used as inputs in another task. After Dask generates these task graphs, it needs to execute them on parallel hardware. This is the job of a task scheduler. Different task schedulers exist, and each will consume a task graph and compute the same result, but with different performance characteristics.\n",
    "> Dask has two families of task schedulers:\n",
    ">\n",
    "> 1.  **Single-machine scheduler**: This scheduler provides basic features on a local process or thread pool. This scheduler was made first and is the default. It is simple and cheap to use, although it can only be used on a single machine and does not scale\n",
    "> 2.  **Distributed scheduler**: This scheduler is more sophisticated, offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster\n",
    ">\n",
    "> &mdash; <cite>https://docs.dask.org/en/stable/scheduling.html</cite>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../_static/dask-overview-schedulers.svg\" alt=\"Dask Schedulers\" style=\"display: inline-block;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a local Dask Distributed Scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray is setup to use Dask's default single-machine, multi-threaded scheduler. However, Dask advises users to use the Dask distributed scheduler for more advanced functionality and more resource-intensive needs.\n",
    "\n",
    "&mdash; <cite>https://docs.dask.org/en/stable/scheduling.html#dask-distributed-local</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup the Dask Client for the local distributed scheduler\n",
    "\n",
    "Xarray will automatically use the Dask Client when calling `.compute()` or `.load()`\n",
    "to trigger queued up operations in the Dask task graph.\n",
    "\n",
    "You can configure the Dask Client (e.g., memory limit) to your needs. Check these\n",
    "API docs out:\n",
    "\n",
    "- https://distributed.dask.org/en/latest/api.html#client\n",
    "- https://distributed.dask.org/en/latest/api.html#distributed.LocalCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Open the Dask Dashboard UI\n",
    "\n",
    "The Dask distributed scheduler provides an interactive dashboard containing many plots\n",
    "and tables with live information.\n",
    "\n",
    "Check this [Dask documentation page](https://docs.dask.org/en/stable/dashboard.html) to learn how to interpret the information. Here's an example:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../_static/dask-dashboard-example.png\" alt=\"Dask Dashboard UI Example\" style=\"display: inline-block; width:800px;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the link to the dashboard\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run an Xarray/xCDAT computation while viewing the dashboards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_daily_avg3 = ds.temporal.average(\"tas\")\n",
    "\n",
    "tas_daily_avg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_daily_avg3.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any other optimizations tips for working with Dask and Xarray?\n",
    "\n",
    "We HIGHLY recommend checking out the [Optimization Tips](https://docs.xarray.dev/en/stable/user-guide/dask.html#optimization-tips) section if you are using Dask with Xarray\n",
    "\n",
    "### Are there cases where xCDAT loads Dask arrays into memory?\n",
    "\n",
    "As of `xarray=2023.5.0`, Xarray does not support updating/setting multi-dimensional dask\n",
    "arrays. The followin error is raised if this is attempted: `xarray can't set arrays with multiple array indices to dask yet`.\n",
    "\n",
    "As a workaround, xCDAT loads coordinate bounds into memory if they are multi-dimensional\n",
    "Dask arrays before performing operations or computations. This loading occurs in the\n",
    "following APIs:\n",
    "\n",
    "- `xcdat.axis.swap_lon_axis`\n",
    "  - swapping longitude axis orientation\n",
    "  - aligning longitude bounds to (0, 360) axis\n",
    "- `xarray.Dataset.spatial.average`\n",
    "  - generating weights using lat/lon coordinate bounds\n",
    "  - swapping longitude axis orientation\n",
    "  - scaling domain bounds to a specified region\n",
    "- `xcdat.Dataset.temporal.<average|group_average|climatology|departures>`\n",
    "  - generating weights using time coordinate bounds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcdat_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
